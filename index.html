<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="Long-Tailed 3D Detection via Multi-Modal Late-Fusion." />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/twitter-teaser-tails.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Long-Tailed 3D Detection via Multi-Modal Late-Fusion.">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/twitter-teaser-tails.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Long-Tailed Distribution, 3D Detection, Multi-Modal Late-Fusion, Autonomous Vehicles, Open World, LiDAR, RGB">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Long-Tailed 3D Detection via Multi-Modal Late-Fusion.</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Long-Tailed 3D Detection via Multi-Modal Late-Fusion
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Yechi Ma</a><sup>*</sup><sup>1,3</sup>,</span>
              <span class="author-block">
                <a target="_blank">Neehar Peri</a><sup>*</sup><sup>2</sup>,</span>
              <span class="author-block">
                <a target="_blank">Shuoquan Wei</a><sup>3</sup>,</span>
              <span class="author-block">
                <a target="_blank">Achal Dave</a><sup>4</sup>,</span>
                <br />  
                <span class="author-block">
                <a target="_blank">Wei Hua</a><sup>1,3</sup>,</span>              
              <span class="author-block">
                <a target="_blank">Yanan Li</a><sup>3</sup>,</span>
              </span>
              <span class="author-block">
                <a target="_blank">Deva Ramanan</a><sup>2</sup>,</span>
              </span>
              <span class="author-block">
                <a target="_blank">Shu Kong</a><sup>5</sup>,</span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Zhejiang University<br></span>
              <span class="author-block"><sup>2</sup>Carnegie Mellon University<br></span>
              <span class="author-block"><sup>3</sup>Zhejiang Lab<br></span>
              <span class="author-block"><sup>4</sup>Antrhopic<br></span>
              <span class="author-block"><sup>5</sup>University of Macau<br></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2401.12425.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/mayechi/lt3d-lf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.12425" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            <!-- </div>
            <h1><strong style="color: red; font-size: x-large;">XXXXXX</strong></h1>
          </div> -->
        </div>
      </div>  
    </div>
  </section>
  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p style="text-align: justify;">
              Contemporary autonomous vehicle (AV) benchmarks have advanced techniques for training 3D detectors, 
              particularly on large-scale LiDAR data. Surprisingly, although semantic class labels naturally follow a long-tailed distribution, 
              existing benchmarks focus on only a few common classes (e.g., pedestrian and car) and neglect many rare classes in-the-tail (e.g., debris and stroller). 
              However, AVs must reliably detect both common and rare classes for safe operation in the open world. We address this challenge by formally studying the problem of Long-Tailed 3D Detection (LT3D), 
              which evaluates all annotated classes, including those in-the-tail. We address LT3D with hierarchical losses that promote feature sharing across common-vs-rare classes, 
              and introduce diagnostic metrics that award partial credit to "reasonable" mistakes respecting the hierarchy (e.g., mistaking a child for an adult). 
              Further, we point out that fine-grained tail class accuracy is particularly improved via multimodal late fusion of independently trained uni-modal LiDAR and RGB detectors. 
              Importantly, such a late-fusion framework allows us to leverage large-scale uni-modal datasets (with more examples for rare classes) to train better uni-modal RGB detectors, 
              unlike prevailing multimodal detectors that require paired multi-modal training data. Finally, we examine three critical components of our simple late-fusion approach from first principles and investigate whether to train 2D or 3D RGB detectors, 
              whether to match RGB and LiDAR detections in 3D or the projected 2D image plane for fusion, and how to fuse matched detections. 
              Extensive experiments reveal that 2D RGB detectors achieve better recognition accuracy for rare classes than 3D RGB detectors and matching on the 2D image plane mitigates depth estimation errors. 
              Our proposed late-fusion approach significantly improves LT3D performance over prior art, particularly improving mAP on rare classes from 12.8 to 20.0!
            </ul>  
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Long-Tailed 3D Benchmarking Protocol</h2>
          <!-- <h2 class="title is-4">Strong Correlation of Concept Frequency and Accuracy</h2> -->
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/nuScenes_Distribution_00.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                We report performance for three groups of classes based on their cardinality (split by dotted lines): Many, Medium, and Few.
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/Hierarchy_v1_00.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                The nuScenes defines a semantic hierarchy for all classes, grouping semantically similar classes under coarse-grained categories.
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">LT3D Methods: The Devil Is In The Details</h2>
          <h2 class="title is-4">Grouping-Free Detector Head</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/arch_v4_00.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                We treat each class as its own group to avoid hand-crafted grouping heuristics and design a group-free strategy, in which each class has only one linear layer as its detector and all classes share a signle detector head.
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Multimodal Filtering in 3D for Detection Fusion</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/Filter_v1_00.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                Multimodal filtering effectively removes false-positive LiDAR detections despite their high confidence scores.
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Delving into Multi-Modal Late-Fusion for LT3D</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/splashy-fig-v3_00.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                We examine three key components in the late-fusion of uni-modal RGB and LiDAR detectors。
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/LateFusionPipeline-v3_00.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                Our multi-modal late-fusion approach takes 3D LiDAR and 2D RGB detections as input, matches 2D RGB and (projected) 3D LiDAR detections on the image plane, and fuses matched predictions with score calibration and probabilistic ensembling to produce 3D detections.
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>


  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Visualizations</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/visual-results-v3_00.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                Three examples demonstrate how our multi-modal late-fusion improves LT3D by ensembling 2D RGB detections (from DINO) and 3D LiDAR detections (from CenterPoint).
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
          <div class="content has-text-justified">

            <div class="item">
              <video src="static/videos/lt3d-lf-video.mp4"></video> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                The video demo shows that our method is significantly better than CMT on dataset.
              </p>
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>





  
  <!-- Qualitative Results -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Benchmarking Results</h2>
          <div class="content">

            <div class="item">
              <!-- Your image here -->
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
                State-of-the-art comparison on nuScenes.
              </p>
              <img src="static/images/Table1.png" alt="1" />
            </div>
            <div class="item">
              <!-- Your image here -->
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;"> 
                State-of-the-art comparison on Argoverse 2.0.
              </p>
              <img src="static/images/Table2.png" alt="1" style="height: 200px;"/>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Qualitative Results -->

  <!--BibTex citation -->
  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content ">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{yechi2024long-tailed,
        title={Long-Tailed 3D Detection via Multi-Modal Late-Fusion},
        author={Ma, Yechi and Peri, Neehar and Wei, Shuoquan and Dave, Achal and Hua, Wei and Li, Yanan and Ramanan, Deva and Kong, Shu},
        journal={arXiv preprint arXiv:2401.12425},
        year={2024}
      }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->



  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
