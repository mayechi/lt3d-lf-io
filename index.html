<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="Long-Tailed 3D Detection via Multi-Modal Late-Fusion." />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/twitter-teaser-tails.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Long-Tailed 3D Detection via Multi-Modal Late-Fusion.">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/twitter-teaser-tails.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Long-Tailed Distribution, 3D Detection, Multi-Modal Late-Fusion, Autonomous Vehicles, Open World, LiDAR, RGB">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Long-Tailed 3D Detection via Multi-Modal Late-Fusion.</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Long-Tailed 3D Detection via Multi-Modal Late-Fusion
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Yechi Ma</a><sup>*</sup><sup>1,3</sup>,</span>
              <span class="author-block">
                <a target="_blank">Neehar Peri</a><sup>*</sup><sup>2</sup>,</span>
              <span class="author-block">
                <a target="_blank">Shuoquan Wei</a><sup>3</sup>,</span>
              <!-- <span class="author-block">
                <a target="_blank">Achal Dave</a><sup>4</sup>,</span> -->
              <br />
              <span class="author-block">
                <a target="_blank">Wei Hua</a><sup>1,3</sup>,</span>
              <span class="author-block">
                <a target="_blank">Yanan Li</a><sup>3</sup>,</span>
              </span>
              <span class="author-block">
                <a target="_blank">Deva Ramanan</a><sup>2</sup>,</span>
              </span>
              <span class="author-block">
                <a target="_blank">Shu Kong</a><sup>5</sup>,</span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Zhejiang University<br></span>
              <span class="author-block"><sup>2</sup>Carnegie Mellon University<br></span>
              <span class="author-block"><sup>3</sup>Zhejiang Lab<br></span>
              <span class="author-block"><sup>4</sup>Antrhopic<br></span>
              <span class="author-block"><sup>5</sup>University of Macau<br></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2401.12425.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/mayechi/lt3d-lf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.12425" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
            <h1><strong style="color: red; font-size: x-large;">XXXXXX</strong></h1>
          </div>
        </div>
      </div>  
    </div>
  </section>
  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p style="text-align: justify;">
              Contemporary autonomous vehicle (AV) benchmarks have advanced techniques for training 3D detectors, 
              particularly on large-scale LiDAR data. Surprisingly, although semantic class labels naturally follow a long-tailed distribution, 
              existing benchmarks focus on only a few common classes (e.g., pedestrian and car) and neglect many rare classes in-the-tail (e.g., debris and stroller). 
              However, AVs must reliably detect both common and rare classes for safe operation in the open world. We address this challenge by formally studying the problem of Long-Tailed 3D Detection (LT3D), 
              which evaluates all annotated classes, including those in-the-tail. We address LT3D with hierarchical losses that promote feature sharing across common-vs-rare classes, 
              and introduce diagnostic metrics that award partial credit to "reasonable" mistakes respecting the hierarchy (e.g., mistaking a child for an adult). 
              Further, we point out that fine-grained tail class accuracy is particularly improved via multimodal late fusion of independently trained uni-modal LiDAR and RGB detectors. 
              Importantly, such a late-fusion framework allows us to leverage large-scale uni-modal datasets (with more examples for rare classes) to train better uni-modal RGB detectors, 
              unlike prevailing multimodal detectors that require paired multi-modal training data. Finally, we examine three critical components of our simple late-fusion approach from first principles and investigate whether to train 2D or 3D RGB detectors, 
              whether to match RGB and LiDAR detections in 3D or the projected 2D image plane for fusion, and how to fuse matched detections. 
              Extensive experiments reveal that 2D RGB detectors achieve better recognition accuracy for rare classes than 3D RGB detectors and matching on the 2D image plane mitigates depth estimation errors. 
              Our proposed late-fusion approach significantly improves LT3D performance over prior art, particularly improving mAP on rare classes from 12.8 to 20.0!
            </ul>  
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Long-Tailed 3D Benchmarking Protocol</h2>
          <!-- <h2 class="title is-4">Strong Correlation of Concept Frequency and Accuracy</h2> -->
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/nuScenes_Distribution.pdf" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                we report performance for three groups of classes based on their cardinality (split by dotted lines): Many, Medium, and Few.
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/Hierarchy_v1.pdf" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                The nuScenes defines a semantic hierarchy for all classes, grouping semantically similar classes under coarse-grained categories.
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Method Overview -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Our Findings</h2>
          <h2 class="title is-4">VLMs show imbalanced performance due to a long-tailed concept distribution</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>
              <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/> -->
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
                <div class="item">
    
                <section class="hero is-big">
                  <div class="hero-body">
                    <div class="container" >
                      <div id="results-carousel" class="carousel results-carousel">
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">ImageNet</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container" style="align-items: center;">
                            
                            <div class="image-with-subtitle">
                              <img src="static/images/imagenet_1k_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle" >
                              <img src="static/images/imagenet_1k_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Flowers</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/flowers102_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/flowers102_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Aircraft</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/fgvc_aircraft_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/fgvc_aircraft_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">CUB</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/cub2011_freq.png" alt="1" style="height: 310px;" />
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/cub2011_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Cars</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/stanford_cars_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/stanford_cars_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Pets</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/oxford_pets_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/oxford_pets_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Food</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/food101_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/food101_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>    
    
                      </div>
                    </div>
                  </div>
                </section>

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                <strong>Left</strong>: By analyzing the texts of large-scale pretraining datasets such as LAION-400M (<span style="color: #4b92c3">blue</span>) and LAION-2B (<span style="color: #ff983e">orange</span>), we show that 
                these pretraining datasets exhibit a long-tailed distribution for visual concepts defined in a variety of
                downstream tasks.  
                <br/>
                <br />
                <strong>Right</strong>: We show that for zero-shot recognition, OpenCLIP models trained on LAION-400M (<span style="color: #4b92c3">blue</span>) and LAION-2B (<span style="color: #ff983e">orange</span>) respecitvely
                yield per-class accuracies that strongly correlate with the long-tailed distribution of concept frequencies (binned on a log scale).
                Interestingly, other VLMs such as CLIP (<span style="color: #de5252">red</span>) and MetaCLIP (<span style="color: #56b356">green</span>) (trained on private-data) also show similar imbalanced performance. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Method Overview -->


  <!-- Text-Image generation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Multi-Modal Systems Fail on Rare Concepts</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/dalle3.png" alt="Image showing DALL-E 3 output" /> -->
              <section class="hero is-small">
                <div class="hero-body">
                  <div class="container">
                    <div id="results-carousel" class="carousel results-carousel">
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Tailed Frog</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/tailed_frog_v3.png" alt="1" />
                            <p class="subtitle-text">Tailed Frog</p>
                          </div>
  
                        </div>
  
                      </div>
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Electric Ray</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/electric_ray_v2.png" alt="1" />
                            <p class="subtitle-text">Electric Ray</p>
                          </div>
  
                        </div>
  
                      </div>
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">BAE 146-200</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/boe146200_v2.png" alt="1" />
                            <p class="subtitle-text">BAE 146-200</p>
                          </div>
                      
  
                        </div>
  
                      </div>
  
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Pan Flute</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/pan_flute_v2.png" alt="1"  />
                            <p class="subtitle-text">Pan Flute</p>
                          </div>
                        
  
                        </div>
  
                      </div>
  
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Night Snake</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/night_snake_v2.png" alt="1" />
                            <p class="subtitle-text">Night Snake</p>
                          </div>
  
                        </div>
  
                      </div>
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Longnose Gar</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/longnose_gar_v2.png" alt="1" />
                            <p class="subtitle-text">Longnose Gar</p>
                          </div>
                          
  
                        </div>
  
                      </div>
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Coral Fungus</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/coral_fungus_v2.png" alt="1" />
                            <p class="user-query">Coral Fungus</p>
                          </div>
  
                        </div>
  
                      </div>
  
                      <div class="item">
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/hard_leaved_pocket_orchid_v2.png" alt="1" />
                            <p class="user-query">Hard-leaved Pocket Orchid</p>
                          </div>
  
                        </div>
  
                      </div>
  
  
  
  
  
  
  
  
  
                    </div>
                  </div>
                </div>
              </section>

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
                Our analysis of LAION-400M and LAION-2B helps us identify visual concepts that are under-represented
                in the pretraining datasets of Vision Language models. Further, we show that state-of-the-art multi-modal systems including
                <a href="https://openai.com/research/gpt-4v-system-card">GPT4-V</a>, <a href="https://llava.hliu.cc/">LLaVA</a>, 
                <a href="https://openai.com/dall-e-3">DALL-E 3</a>, and <a href="https://stablediffusionweb.com/">SD-XL</a> all fail to recognize or generate these rare concepts. More examples are shown in our paper.
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prompt Inversion -->


  <!-- Prompt Inversion -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Solutions</h2>
          <div class="content has-text-justified">
            <p>
              To mitigate the imbalanced performance of VLMs we propose a novel prompting solution (REAL-Prompt) and a retrieval-augmented strategy
              REAL-Linear.
            </p>
          </div>
          <h2 class="title is-4">REAL-Prompt</h2>
          <div class="content has-text-justified">
            <img src="static/images/synonyms_serif-v7-compressed.png" alt="Showcasing the efficiency of REAL-Prompt."
                style="height: auto; display: block; margin: 10px;">
              <p>
                  REAL-Prompt replaces the given concept names with their most frequent synonyms (in the pretraining data of VLMs) and constructs prompts for zero-shot recognition. We display some
                  specific concepts from ImageNet, their most frequent synonyms, frequency (in LAION-400M), and their per-class accuracy. Clearly,
                  the simple change in prompts significantly improves zero-shot recognition. 
              </p>
              <div class="item">
                <!-- Your image here -->
                
              </div>
            <section class="hero is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/bank_swallow.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;">
                            <strong>Bank Swallow</strong>, a small bird with brown back and white belly. When prompted with the original concept name (bank swallow), 
                            both DALL-E 3 and SD-XL generate incorrect images of birds with incorrect black backs. However, prompting with the most frequent synonym (sand martin) 
                            guides both systems to produce correct images.
                          </p>
                        </div>
                        
                      </div>
                      
                    </div>

                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/Thorn Apple.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;"><strong>Thorn Apple</strong>, a plant with large, white, trumpet-shaped flowers. When prompted with the original concept name(thorn apple),
                            DALL-E 3 generates an image with sharp thorns along its stem. Even worse, SD-XL takes the names superficially and generates an apple with thorns. On the contrary, prompting with the most frequent synonym (datura) leads to correct images in both systems.</p>
                        </div>
                      </div>
                    </div>

                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/Pocket Orchid.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;"><strong>Hard Leaved Pocket Orchid</strong>, a type of orchid with a distinctive pouch and symmetrical large petals. When prompted with the original conceptname(hard leaved pocket orchid),
                            both DALL-E 3 and SD-XL generate incorrect images (note the missing pocket and shape of the petals). However, when prompted with the most frequent synonym (Paphiopedilum micranthum), 
                            DALL-E 3 produces the correct image. In contrast, SD-XL can recover the shape of petals but still misses the pocket.</p>
                        </div>
                      </div>
                    </div>

                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/aeroplane.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;"><strong>BAE 146-200</strong>,
                            an airplane with 4 engines. When prompted with the original concept name (BAE 146-200), both DALL-E 3 and SD-XL fail by generating only 2 engines. DALL-E 3 also mistakenly generates extra wings sometimes!
                            However, when prompted with the most frequent synonym (avro rj85), both can generate correct images with 4 engines.</p>
                        </div>
                      </div>
                    </div>

                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/keyboard.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;">keyboard space bar,
                            a long bar at the bottom of a computer keyboard. When prompted with the original concept name (keyboard space bar), 
                            both DALL-E 3 and SD-XL fail by focusing on generating images of the keyboard. However, when prompted with the most frequent synonyms (space bar), both can generate correct images.</p>
                        </div>
                      </div>
                    </div>
                    </div>
                  </div>
                </div>
              
            </section>
            <div class="item">
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                We show some tailed concepts where DALL-E 3 and SD-XL can fail to generate correct images when using the original concept name.
                However using REAL-Prompt, which uses the most frequent synonym to construct prompts, can help produce correct images.
              </p>
            </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prompt Inversion -->

  <!-- Quantitative Results -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">REAL-Linear</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/real_linear_final_new.png" alt="Image showing DALL-E 3 output" />
            </div>
            <p>
              To further improve the zero-shot recognition performance of VLMs, we propose REAL-Linear, a lightweight yet powerful retrieval-augmented solution. REAL-Linear uses all synonyms of 
              the given concepts to retrieve a class-balanced subset of pretraining images (e.g., 500 images per class from the
              dataset LAION-400M). These steps have been demonstrated above. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Quantitative Results -->

  <!-- Qualitative Results -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Benchmarking REAL</h2>
          <div class="content">

            <div class="item">
              <!-- Your image here -->
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
                Within the zero-shot prompting paradigm, our REAL-Prompt outperforms existing prompting approaches such as <a href="https://arxiv.org/abs/2210.07183">DCLIP</a>, and <a href="https://arxiv.org/abs/2209.03320">CuPL</a>. 
                Next, we show that our REAL-Linear rivals the recent retrieval augmented method <a href="https://react-vl.github.io/">REACT</a>. The best numbers are highlighted in bold, second best are underlined.
              </p>
              <img src="static/images/real-benchmark.png" alt="Image showing DALL-E 3 output" />
            </div>
            <div class="item">
              <!-- Your image here -->
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;"> 
                We show that for ImageNet REAL-Linear only uses <strong>5%</strong> of retrieved images and <strong>1%</strong> of compute as compared to recent state-of-the-art REACT.
              </p>
              <img src="static/images/real_linear_efficient.png" alt="Image showing DALL-E 3 output" style="height: 200px;"/>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Qualitative Results -->

  <!--BibTex citation -->
  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content ">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{yechi2024long-tailed,
        title={Long-Tailed 3D Detection via Multi-Modal Late-Fusion},
        author={Ma, Yechi and Peri, Neehar and Wei, Shuoquan and HUa, Wei and Li, Yanan and Ramanan, Deva and Kong, Shu},
        journal={arXiv preprint arXiv:2401.12425},
        year={2024}
      }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->



  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
