<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="The Neglected Tails of Vision Language Models." />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="The Neglected Tails of Vision Language Models.">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>The Neglected Tails of Vision-Language Models.</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">The Neglected Tails of Vision Language Models
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Shubham Parashar</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Zhiqiu Lin</a><sup>*</sup><sup>2</sup>,</span>
              <span class="author-block">
                <a target="_blank">Tian Liu</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Xiangjue Dong</a><sup>1</sup>,</span>
              <!-- <span class="author-block">
                <a target="_blank">Tiffany Ling</a><sup>2</sup>,</span> -->
              <br />
              <span class="author-block">
                <a target="_blank">Yanan Li</a><sup>4</sup>,</span>
              <span class="author-block">
                <a target="_blank">Deva Ramanan</a><sup>2</sup>,</span>
              </span>
              <span class="author-block">
                <a target="_blank">James Caverlee</a><sup>1</sup>,</span>
              </span>
              <span class="author-block">
                <a target="_blank">Shu Kong</a><sup>1</sup><sup>,</sup><sup>3</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Texas A&M University<br></span>
              <span class="author-block"><sup>2</sup>Carnegie Mellon University<br></span>
              <span class="author-block"><sup>3</sup>University of Macau<br></span>
              <span class="author-block"><sup>4</sup>Zhejiang Lab<br></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2401.12425.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/shubhamprshr27/NeglectedTailsVLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.12425" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p style="text-align: justify;">
              Vision-language models (VLMs) excel in zero-shot
recognition but their performance varies greatly across
different visual concepts. For example, although CLIP
achieves impressive accuracy on ImageNet (60-80%), its
performance drops below 10% for more than ten concepts
like night snake, presumably due to their limited presence in the pretraining data. However, measuring the frequency of concepts in VLMs’ large-scale datasets is challenging. We address this by using large language models
(LLMs) to count the number of pretraining texts that contain synonyms of these concepts. Our analysis confirms
that popular datasets, such as LAION, exhibit a long-tailed
concept distribution, yielding biased performance in VLMs.
We also find that downstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and text-to-image models
(e.g., Stable Diffusion), often fail to recognize or generate
images of rare concepts identified by our method. To mitigate the imbalanced performance of zero-shot VLMs, we
propose REtrieval-Augmented Learning (REAL). First, instead of prompting VLMs using the original class names,
REAL uses their most frequent synonyms found in pretraining texts. This simple change already outperforms costly
human-engineered and LLM-enriched prompts over nine
benchmark datasets. Second, REAL trains a linear classifier on a small yet balanced set of pretraining data retrieved using concept synonyms. REAL surpasses the previous zero-shot SOTA, using 400× less storage and 10,000×
less training time!
            </ul>  
              <!-- In this work, we make the first attempt to measure 
              the concept frequency in VLMs' pretraining data by analyzing pretraining 
              texts. We use off-the-shelf language models to help count relevant texts that 
              contain synonyms of the given concepts and resolve ambiguous cases. Our analysis 
              confirms that popular VLM datasets like LAION indeed exhibit a long-tailed 
              concept distribution, which strongly correlates with per-class accuracies. 
              Further, mainstream multimodal systems, including visual chatbots and 
              text-to-image models, struggle with the rare concepts identified by our method. 
              Next, to mitigate VLMs' imbalanced performance in zero-shot recognition, we 
              propose <strong>RE</strong>trieval-<strong>A</strong>ugmented <strong>L</strong>earning <strong>(REAL)</strong>. First, instead of prompting VLMs 
              using the original class names defined in a downstream task, REAL uses their 
              most frequent synonyms found in the pretraining texts. This already outperforms 
              human-engineered and LLM-generated prompts over nine benchmark datasets, likely 
              because VLMs have seen more images associated with the frequently used synonyms. 
              Second, REAL uses all the concept synonyms to retrieve a small, class-balanced 
              set of pretraining data to train a robust classifier. REAL surpasses the recent 
              retrieval-augmented solution REACT, using 400 times less storage and 10,000 
              times less training time! -->
            <!-- </p> -->
          
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Measuring Concept Frequency in Pretraining Data</h2>
          <!-- <h2 class="title is-4">Strong Correlation of Concept Frequency and Accuracy</h2> -->
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/tiger.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                We use LLMs such as ChatGPT to help count texts relevant to the concept of interest, as visually illustrated above for the concept of "tiger".
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Method Overview -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Our Findings</h2>
          <h2 class="title is-4">VLMs show imbalanced performance due to a long-tailed concept distribution</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>
              <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/> -->
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
                <div class="item">
    
                <section class="hero is-big">
                  <div class="hero-body">
                    <div class="container" >
                      <div id="results-carousel" class="carousel results-carousel">
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">ImageNet</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container" style="align-items: center;">
                            
                            <div class="image-with-subtitle">
                              <img src="static/images/imagenet_1k_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle" >
                              <img src="static/images/imagenet_1k_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Flowers</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/flowers102_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/flowers102_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Aircraft</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/fgvc_aircraft_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/fgvc_aircraft_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">CUB</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/cub2011_freq.png" alt="1" style="height: 310px;" />
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/cub2011_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Cars</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/stanford_cars_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/stanford_cars_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Pets</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/oxford_pets_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/oxford_pets_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>
    
                        <div class="item">
    
                          <div class="query-container">
                            <p class="user-query">Food</p>
                          </div>
    
                          <!-- Image container -->
                          <div class="images-container">
    
                            <div class="image-with-subtitle">
                              <img src="static/images/food101_freq.png" alt="1" style="height: 310px;"/>
                              <p class="subtitle-text">Frequency Distribution</p>
                            </div>
                            <div class="image-with-subtitle">
                              <img src="static/images/food101_acc.png" alt="2" style="height: 310px;"/>
                              <p class="subtitle-text">Zero-Shot Accuracy</p>
                            </div>
    
                          </div>
    
                        </div>    
    
                      </div>
                    </div>
                  </div>
                </section>

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                <strong>Left</strong>: By analyzing the texts of large-scale pretraining datasets such as LAION-400M (<span style="color: #4b92c3">blue</span>) and LAION-2B (<span style="color: #ff983e">orange</span>), we show that 
                these pretraining datasets exhibit a long-tailed distribution for visual concepts defined in a variety of
                downstream tasks.  
                <br/>
                <br />
                <strong>Right</strong>: We show that for zero-shot recognition, OpenCLIP models trained on LAION-400M (<span style="color: #4b92c3">blue</span>) and LAION-2B (<span style="color: #ff983e">orange</span>) respecitvely
                yield per-class accuracies that strongly correlate with the long-tailed distribution of concept frequencies (binned on a log scale).
                Interestingly, other VLMs such as CLIP (<span style="color: #de5252">red</span>) and MetaCLIP (<span style="color: #56b356">green</span>) (trained on private-data) also show similar imbalanced performance. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Method Overview -->


  <!-- Text-Image generation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Multi-Modal Systems Fail on Rare Concepts</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/dalle3.png" alt="Image showing DALL-E 3 output" /> -->
              <section class="hero is-small">
                <div class="hero-body">
                  <div class="container">
                    <div id="results-carousel" class="carousel results-carousel">
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Tailed Frog</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/tailed_frog_v3.png" alt="1" />
                            <p class="subtitle-text">Tailed Frog</p>
                          </div>
  
                        </div>
  
                      </div>
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Electric Ray</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/electric_ray_v2.png" alt="1" />
                            <p class="subtitle-text">Electric Ray</p>
                          </div>
  
                        </div>
  
                      </div>
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">BAE 146-200</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/boe146200_v2.png" alt="1" />
                            <p class="subtitle-text">BAE 146-200</p>
                          </div>
                      
  
                        </div>
  
                      </div>
  
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Pan Flute</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/pan_flute_v2.png" alt="1"  />
                            <p class="subtitle-text">Pan Flute</p>
                          </div>
                        
  
                        </div>
  
                      </div>
  
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Night Snake</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/night_snake_v2.png" alt="1" />
                            <p class="subtitle-text">Night Snake</p>
                          </div>
  
                        </div>
  
                      </div>
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Longnose Gar</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/longnose_gar_v2.png" alt="1" />
                            <p class="subtitle-text">Longnose Gar</p>
                          </div>
                          
  
                        </div>
  
                      </div>
  
                      <div class="item">
  
                        <!-- <div class="query-container">
                          <p class="user-query">Coral Fungus</p>
                        </div> -->
  
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/coral_fungus_v2.png" alt="1" />
                            <p class="user-query">Coral Fungus</p>
                          </div>
  
                        </div>
  
                      </div>
  
                      <div class="item">
                        <!-- Image container -->
                        <div class="images-container">
  
                          <div class="image-with-subtitle">
                            <img src="static/images/hard_leaved_pocket_orchid_v2.png" alt="1" />
                            <p class="user-query">Hard-leaved Pocket Orchid</p>
                          </div>
  
                        </div>
  
                      </div>
  
  
  
  
  
  
  
  
  
                    </div>
                  </div>
                </div>
              </section>

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
                Our analysis of LAION-400M and LAION-2B helps us identify visual concepts that are under-represented
                in the pretraining datasets of Vision Language models. Further, we show that state-of-the-art multi-modal systems including
                <a href="https://openai.com/research/gpt-4v-system-card">GPT4-V</a>, <a href="https://llava.hliu.cc/">LLaVA</a>, 
                <a href="https://openai.com/dall-e-3">DALL-E 3</a>, and <a href="https://stablediffusionweb.com/">SD-XL</a> all fail to recognize or generate these rare concepts. More examples are shown in our paper.
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prompt Inversion -->


  <!-- Prompt Inversion -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Solutions</h2>
          <div class="content has-text-justified">
            <p>
              To mitigate the imbalanced performance of VLMs we propose a novel prompting solution (REAL-Prompt) and a retrieval-augmented strategy
              REAL-Linear.
            </p>
          </div>
          <h2 class="title is-4">REAL-Prompt</h2>
          <div class="content has-text-justified">
            <img src="static/images/synonyms_serif-v7-compressed.png" alt="Showcasing the efficiency of REAL-Prompt."
                style="height: auto; display: block; margin: 10px;">
              <p>
                  REAL-Prompt replaces the given concept names with their most frequent synonyms (in the pretraining data of VLMs) and constructs prompts for zero-shot recognition. We display some
                  specific concepts from ImageNet, their most frequent synonyms, frequency (in LAION-400M), and their per-class accuracy. Clearly,
                  the simple change in prompts significantly improves zero-shot recognition. 
              </p>
              <div class="item">
                <!-- Your image here -->
                
              </div>
            <section class="hero is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/bank_swallow.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;">
                            <strong>Bank Swallow</strong>, a small bird with brown back and white belly. When prompted with the original concept name (bank swallow), 
                            both DALL-E 3 and SD-XL generate incorrect images of birds with incorrect black backs. However, prompting with the most frequent synonym (sand martin) 
                            guides both systems to produce correct images.
                          </p>
                        </div>
                        
                      </div>
                      
                    </div>

                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/Thorn Apple.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;"><strong>Thorn Apple</strong>, a plant with large, white, trumpet-shaped flowers. When prompted with the original concept name(thorn apple),
                            DALL-E 3 generates an image with sharp thorns along its stem. Even worse, SD-XL takes the names superficially and generates an apple with thorns. On the contrary, prompting with the most frequent synonym (datura) leads to correct images in both systems.</p>
                        </div>
                      </div>
                    </div>

                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/Pocket Orchid.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;"><strong>Hard Leaved Pocket Orchid</strong>, a type of orchid with a distinctive pouch and symmetrical large petals. When prompted with the original conceptname(hard leaved pocket orchid),
                            both DALL-E 3 and SD-XL generate incorrect images (note the missing pocket and shape of the petals). However, when prompted with the most frequent synonym (Paphiopedilum micranthum), 
                            DALL-E 3 produces the correct image. In contrast, SD-XL can recover the shape of petals but still misses the pocket.</p>
                        </div>
                      </div>
                    </div>

                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/aeroplane.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;"><strong>BAE 146-200</strong>,
                            an airplane with 4 engines. When prompted with the original concept name (BAE 146-200), both DALL-E 3 and SD-XL fail by generating only 2 engines. DALL-E 3 also mistakenly generates extra wings sometimes!
                            However, when prompted with the most frequent synonym (avro rj85), both can generate correct images with 4 engines.</p>
                        </div>
                      </div>
                    </div>

                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/keyboard.png" alt="1" />
                          <p class="subtitle-text" style="text-align: justify;">keyboard space bar,
                            a long bar at the bottom of a computer keyboard. When prompted with the original concept name (keyboard space bar), 
                            both DALL-E 3 and SD-XL fail by focusing on generating images of the keyboard. However, when prompted with the most frequent synonyms (space bar), both can generate correct images.</p>
                        </div>
                      </div>
                    </div>
                    </div>
                  </div>
                </div>
              
            </section>
            <div class="item">
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                We show some tailed concepts where DALL-E 3 and SD-XL can fail to generate correct images when using the original concept name.
                However using REAL-Prompt, which uses the most frequent synonym to construct prompts, can help produce correct images.
              </p>
            </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prompt Inversion -->

  <!-- Quantitative Results -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">REAL-Linear</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/real_linear_final_new.png" alt="Image showing DALL-E 3 output" />
            </div>
            <p>
              To further improve the zero-shot recognition performance of VLMs, we propose REAL-Linear, a lightweight yet powerful retrieval-augmented solution. REAL-Linear uses all synonyms of 
              the given concepts to retrieve a class-balanced subset of pretraining images (e.g., 500 images per class from the
              dataset LAION-400M). These steps have been demonstrated above. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Quantitative Results -->

  <!-- Qualitative Results -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Benchmarking REAL</h2>
          <div class="content">

            <div class="item">
              <!-- Your image here -->
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
                Within the zero-shot prompting paradigm, our REAL-Prompt outperforms existing prompting approaches such as <a href="https://arxiv.org/abs/2210.07183">DCLIP</a>, and <a href="https://arxiv.org/abs/2209.03320">CuPL</a>. 
                Next, we show that our REAL-Linear rivals the recent retrieval augmented method <a href="https://react-vl.github.io/">REACT</a>. The best numbers are highlighted in bold, second best are underlined.
              </p>
              <img src="static/images/real-benchmark.png" alt="Image showing DALL-E 3 output" />
            </div>
            <div class="item">
              <!-- Your image here -->
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;"> 
                We show that for ImageNet REAL-Linear only uses <strong>5%</strong> of retrieved images and <strong>1%</strong> of compute as compared to recent state-of-the-art REACT.
              </p>
              <img src="static/images/real_linear_efficient.png" alt="Image showing DALL-E 3 output" style="height: 200px;"/>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Qualitative Results -->

  <!--BibTex citation -->
  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content ">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{parashar2024neglected, <!--Fill once available-->
        title={The Neglected Tails of Vision Language Models.}, 
        author={Shubham Parashar and Zhiqiu Lin and Tian Liu and Xiangjue Dong and Yanan Li and Deva Ramanan and James Caverlee and Shu Kong},
        year={2023}, <!--Fill once available-->
        eprint={2309.05950}, <!--Fill once available-->
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->



  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
